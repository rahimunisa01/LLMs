{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###PA4: Prompt Engineering"
      ],
      "metadata": {
        "id": "LRlf5YFb0LM5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this exercise, you will perform prompt engineering on a dialogue summarization task using [Flan-T5](https://huggingface.co/google/flan-t5-large) and the [dialogsum dataset](https://huggingface.co/datasets/knkarthick/dialogsum). You will explore how different prompts affect the output of the model, and compare zero-shot and few-shot inferences. <br/>\n",
        "Complete the code in the cells below."
      ],
      "metadata": {
        "id": "BUS6HMPx0Ifd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Set up Required Dependencies"
      ],
      "metadata": {
        "id": "SVx5_AZbY8Uy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "fVcRstMKYivz"
      },
      "outputs": [],
      "source": [
        "!pip3 install datasets -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "yLcnmKQ4Y4m3"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Explore the Dataset"
      ],
      "metadata": {
        "id": "9IMxLPPGY_R7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('knkarthick/dialogsum')"
      ],
      "metadata": {
        "id": "pNk1X7wLZB6I"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print several dialogues with their baseline summaries."
      ],
      "metadata": {
        "id": "6grOvefFZJG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_indices = [0, 42, 800]\n",
        "dash_line = '-' * 100\n",
        "\n",
        "for i, index in enumerate(example_indices):\n",
        "    print(dash_line)\n",
        "    print('Example', i + 1)\n",
        "    print(dash_line)\n",
        "    print('INPUT DIALOGUE:')\n",
        "    print(dataset['test'][index]['dialogue'])\n",
        "    print(dash_line)\n",
        "    print('BASELINE HUMAN SUMMARY:')\n",
        "    print(dataset['test'][index]['summary'])\n",
        "    print(dash_line)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pTXCOFoZKtu",
        "outputId": "9041fc58-61cf-45ed-8764-f29ac8a86eda"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "Example 1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "INPUT DIALOGUE:\n",
            "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
            "#Person2#: Yes, sir...\n",
            "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
            "#Person2#: Yes, sir. Go ahead.\n",
            "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
            "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
            "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
            "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
            "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
            "#Person2#: This applies to internal and external communications.\n",
            "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
            "#Person2#: Is that all?\n",
            "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Example 2\n",
            "----------------------------------------------------------------------------------------------------\n",
            "INPUT DIALOGUE:\n",
            "#Person1#: I don't know how to adjust my life. Would you give me a piece of advice?\n",
            "#Person2#: You look a bit pale, don't you?\n",
            "#Person1#: Yes, I can't sleep well every night.\n",
            "#Person2#: You should get plenty of sleep.\n",
            "#Person1#: I drink a lot of wine.\n",
            "#Person2#: If I were you, I wouldn't drink too much.\n",
            "#Person1#: I often feel so tired.\n",
            "#Person2#: You better do some exercise every morning.\n",
            "#Person1#: I sometimes find the shadow of death in front of me.\n",
            "#Person2#: Why do you worry about your future? You're very young, and you'll make great contribution to the world. I hope you take my advice.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# wants to adjust #Person1#'s life and #Person2# suggests #Person1# be positive and stay healthy.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Example 3\n",
            "----------------------------------------------------------------------------------------------------\n",
            "INPUT DIALOGUE:\n",
            "#Person1#: Dad, you keep talking about family in New Zealand. Who are they?\n",
            "#Person2#: Well, that's your uncle Bill, his wife and two of their daughters.\n",
            "#Person1#: Is uncle Bill your brother?\n",
            "#Person2#: No, your uncle Jack is my brother, Bill is my brother-in-law, your mom's brother.\n",
            "#Person1#: So his two daughters are my cousins?\n",
            "#Person2#: That's right, Sarah and Jane are both your cousins although they are step-sisters.\n",
            "#Person1#: What are step-sisters?\n",
            "#Person2#: Sarah is your uncle Bill's older daughter. When she was young, Bill's first wife, Sarah's mom died. Three years later Bill married again.\n",
            "#Person1#: So uncle Bill's wife is Jane's mother but not Sarah's mother. Right?\n",
            "#Person2#: Yes. She is Sarah's step-mother.\n",
            "#Person1#: And when are they coming to visit us?\n",
            "#Person2#: They want to travel to Europe next year, and will visit us at the same Ae.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person2# tells #Person1# about the relationships between their family and the uncle Bill's, who will visit them next year.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Summarize Dialogues without Prompt Engineering"
      ],
      "metadata": {
        "id": "hYe62oL1Zvnm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the Flan-T5-large model and its tokenizer."
      ],
      "metadata": {
        "id": "n9_9SDRRZw3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'google/flan-t5-large'\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "B2CRPD_1Z0DE"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**: Use the pre-trained model to summarize the example dialogues without any prompt engineering. Use the `model.generate()` function with `max_new_tokens=50`."
      ],
      "metadata": {
        "id": "lc6vMcW_aA5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNayXzvtgErh",
        "outputId": "d76ab75b-f50f-447b-8b4a-8e07ed6d068c"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
              "        num_rows: 12460\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
              "        num_rows: 500\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
              "        num_rows: 1500\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, index in enumerate(example_indices):\n",
        "    dialogue = dataset['test'][index]['dialogue']\n",
        "    inputs = tokenizer(dialogue, return_tensors='pt', truncation=True)\n",
        "\n",
        "\n",
        "    summary_ids = model.generate(inputs['input_ids'], max_new_tokens=50)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    print(dash_line)\n",
        "    print('BASELINE HUMAN SUMMARY:')\n",
        "    print(dataset['test'][index]['summary'])\n",
        "    print(dash_line)\n",
        "    print()\n",
        "\n",
        "    print(dash_line)\n",
        "    print('Example', i + 1)\n",
        "    print(dash_line)\n",
        "    print('MODEL GENERATED SUMMARY:')\n",
        "    print(summary)\n",
        "    print(dash_line)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLnuNLoi1eS3",
        "outputId": "6af586fb-0166-4381-d2ba-12ac2e813ccd"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Example 1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATED SUMMARY:\n",
            "#Person1: Ms. Dawson, please take dictation for me.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# wants to adjust #Person1#'s life and #Person2# suggests #Person1# be positive and stay healthy.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Example 2\n",
            "----------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATED SUMMARY:\n",
            "#Person1#: Thank you, #Person2#.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person2# tells #Person1# about the relationships between their family and the uncle Bill's, who will visit them next year.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Example 3\n",
            "----------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATED SUMMARY:\n",
            "#Person1#: That's great.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that the model generations make some sense, but the model doesn't seem to be sure what task it is supposed to accomplish and it often just makes up the next sentence in the dialogue. Prompt engineering can help here."
      ],
      "metadata": {
        "id": "1EXO78vBoDJs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Summarize Dialogues with Instruction Prompts\n",
        "\n",
        "In order to instruct the model to perform a task (e.g., summarize a dialogue), you can take the dialogue and convert it into an instruction prompt. This is often called **zero-shot inference**."
      ],
      "metadata": {
        "id": "uH0gEQ6UoGT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**: Wrap the dialogues in a descriptive instruction (e.g., \"Summarize the following conversation.\"), and examine how the generated text changes."
      ],
      "metadata": {
        "id": "2saOQHiUoJkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, index in enumerate(example_indices):\n",
        "    dialogue = dataset['test'][index]['dialogue']\n",
        "\n",
        "    # Wrap dialogue in a descriptive instruction\n",
        "    input_text = \"Summarize the following conversation: \" + dialogue\n",
        "\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer(input_text, return_tensors='pt', truncation=True)\n",
        "\n",
        "    summary_ids = model.generate(inputs['input_ids'], max_new_tokens=50)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    print(dash_line)\n",
        "    print('BASELINE HUMAN SUMMARY:')\n",
        "    print(dataset['test'][index]['summary'])\n",
        "    print(dash_line)\n",
        "    print()\n",
        "\n",
        "    print(dash_line)\n",
        "    print('Example', i + 1)\n",
        "    print(dash_line)\n",
        "    print('MODEL GENERATED SUMMARY:')\n",
        "    print(summary)\n",
        "    print(dash_line)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qolNIevXoEB0",
        "outputId": "f78a709e-b0bc-43ef-928d-3af83242dc87"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Example 1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATED SUMMARY:\n",
            "#Person1# wants Ms. Dawson to take dictation for him.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# wants to adjust #Person1#'s life and #Person2# suggests #Person1# be positive and stay healthy.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Example 2\n",
            "----------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATED SUMMARY:\n",
            "#Person1# is worried about his future. #Person2# gives him some advice.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person2# tells #Person1# about the relationships between their family and the uncle Bill's, who will visit them next year.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Example 3\n",
            "----------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATED SUMMARY:\n",
            "Dad, you keep talking about your uncle Bill, his wife and two of their daughters in New Zealand.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is much better! But the model still does not pick up on the nuance of the conversations though."
      ],
      "metadata": {
        "id": "zVMQrT_npLtW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Exercise:** Experiment with the prompt text and see how it influences the generated output. Do the inferences change if you end the prompt with just empty string vs. `Summary: `?"
      ],
      "metadata": {
        "id": "mLFgJ8wb4fZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summaries(prompt_text, description):\n",
        "    print(f\"{description}\")\n",
        "    for i, index in enumerate(example_indices):\n",
        "        dialogue = dataset['test'][index]['dialogue']\n",
        "\n",
        "        # Prepare input text with the specified prompt\n",
        "        input_text = prompt_text + dialogue\n",
        "\n",
        "        # Tokenize the input\n",
        "        inputs = tokenizer(input_text, return_tensors='pt', truncation=True)\n",
        "\n",
        "\n",
        "        summary_ids = model.generate(inputs['input_ids'], max_new_tokens=50)\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        print(dash_line)\n",
        "        print('BASELINE HUMAN SUMMARY:')\n",
        "        print(dataset['test'][index]['summary'])\n",
        "        print(dash_line)\n",
        "        print('MODEL GENERATED SUMMARY:')\n",
        "        print(summary)\n",
        "        print(dash_line)\n",
        "        print()\n",
        "\n",
        "# Experiment with different prompts\n",
        "generate_summaries(\"\", \"Empty String Prompt\")\n",
        "generate_summaries(\"Summary: \", \"Summary: Prompt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eIp0sRCpPBo",
        "outputId": "3ae4ae68-d34c-4ef1-e192-055d49c83bf7"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty String Prompt\n",
            "----------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATED SUMMARY:\n",
            "#Person1: Ms. Dawson, please take dictation for me.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# wants to adjust #Person1#'s life and #Person2# suggests #Person1# be positive and stay healthy.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATED SUMMARY:\n",
            "#Person1#: Thank you, #Person2#.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person2# tells #Person1# about the relationships between their family and the uncle Bill's, who will visit them next year.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATED SUMMARY:\n",
            "#Person1#: That's great.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Summary: Prompt\n",
            "----------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATED SUMMARY:\n",
            "#Person1 wants Ms. Dawson to take dictation for him.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# wants to adjust #Person1#'s life and #Person2# suggests #Person1# be positive and stay healthy.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATED SUMMARY:\n",
            "#Person1 is worried about his future. #Person2 gives him some advice.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person2# tells #Person1# about the relationships between their family and the uncle Bill's, who will visit them next year.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATED SUMMARY:\n",
            "Dad, you keep talking about your uncle Bill, his wife and two of their daughters in New Zealand.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the \"Summary: \" string prompt the model generates summary instead of replying as dialogues in the no string prompt."
      ],
      "metadata": {
        "id": "3OgXRawI5Mbo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise:** Flan-T5 has many prompt templates that are published for certain tasks [here](https://github.com/google-research/FLAN/blob/main/flan/v2/templates.py). Try using its pre-built prompts for dialogue summarization (e.g., the ones under the `\"samsum\"` key) and see how they influence the outputs.\n"
      ],
      "metadata": {
        "id": "R78PDxbWqmaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "samsum_prompts = [\n",
        "        \"{dialogue}\\n\\nBriefly summarize that dialogue.\",\n",
        "        \"Here is a dialogue:\\n{dialogue}\\n\\nWrite a short summary!\",\n",
        "        \"Dialogue:\\n{dialogue}\\n\\nWhat is a summary of this dialogue?\",\n",
        "        \"{dialogue}\\n\\nWhat was that dialogue about, in two sentences or less?\",\n",
        "        \"Here is a dialogue:\\n{dialogue}\\n\\nWhat were they talking about?\",\n",
        "        \"Dialogue:\\n{dialogue}\\nWhat were the main points in that \"\n",
        "         \"conversation?\",\n",
        "        \"Dialogue:\\n{dialogue}\\nWhat was going on in that conversation?\",\n",
        "    ]"
      ],
      "metadata": {
        "id": "Fg_3U8127Rn9"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summaries(prompt_template, description):\n",
        "    print(f\"\\n{'='*20} {description} {'='*20}\\n\")\n",
        "    for i, index in enumerate(example_indices):\n",
        "        dialogue = dataset['test'][index]['dialogue']\n",
        "        human_summary = dataset['test'][index]['summary']\n",
        "\n",
        "        # Prepare input text with the specified prompt template\n",
        "        input_text = prompt_template.format(dialogue=dialogue, summary=human_summary)\n",
        "\n",
        "        # Tokenize the input\n",
        "        inputs = tokenizer(input_text, return_tensors='pt', truncation=True)\n",
        "\n",
        "        # Generate summary with max_new_tokens=50\n",
        "\n",
        "        summary_ids = model.generate(inputs['input_ids'], max_new_tokens=50)\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        print(dash_line)\n",
        "        print('BASELINE HUMAN SUMMARY:')\n",
        "        print(dataset['test'][index]['summary'])\n",
        "        print(dash_line)\n",
        "        print('MODEL GENERATED SUMMARY:')\n",
        "        print(summary)\n",
        "        print(dash_line)\n",
        "        print()\n",
        "\n",
        "# Experiment with different SamSum prompts\n",
        "for i, prompt_template in enumerate(samsum_prompts):\n",
        "    generate_summaries(prompt_template, f\"SamSum Prompt {i+1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrQW1fRaqcq6",
        "outputId": "257c87b4-3d40-4826-a73b-cf7c4d6622f2"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================== SamSum Prompt 1 ====================\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATED SUMMARY:\n",
            "Person1 wants Ms. Dawson to take dictation for him. The new policy restricts all office communications to email correspondence and official memos. Any employee who persists in using Instant Messaging will receive a warning and be placed on\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# wants to adjust #Person1#'s life and #Person2# suggests #Person1# be positive and stay healthy.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATED SUMMARY:\n",
            "Person1 is worried about his future. He should get plenty of sleep, drink less wine and exercise.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person2# tells #Person1# about the relationships between their family and the uncle Bill's, who will visit them next year.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATED SUMMARY:\n",
            "Dad keeps talking about his uncle Bill, his wife and two of their daughters in New Zealand. Sarah and Jane are his cousins. They want to travel to Europe next year and will visit them at the same Ae.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "==================== SamSum Prompt 2 ====================\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATED SUMMARY:\n",
            "Person1 wants to send an intra-office memo to all employees. It's about a new policy on communications. Employees who use Instant Messaging during working hours will be warned and placed on probation. Employees who continue to use Instant\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# wants to adjust #Person1#'s life and #Person2# suggests #Person1# be positive and stay healthy.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATED SUMMARY:\n",
            "Person1 is worried about his future. He doesn't know how to adjust his life. Person2 gives him some advice.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person2# tells #Person1# about the relationships between their family and the uncle Bill's, who will visit them next year.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATED SUMMARY:\n",
            "Dad keeps talking about his uncle Bill, his wife and two of their daughters in New Zealand.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "==================== SamSum Prompt 3 ====================\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATED SUMMARY:\n",
            "Person1 wants Ms. Dawson to take dictation for him. The new policy restricts all office communications to email correspondence and official memos. Any employee who persists in using Instant Messaging will receive a warning and be placed on\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# wants to adjust #Person1#'s life and #Person2# suggests #Person1# be positive and stay healthy.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATED SUMMARY:\n",
            "Person1 is worried about his future. He should get plenty of sleep, drink less wine and exercise.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person2# tells #Person1# about the relationships between their family and the uncle Bill's, who will visit them next year.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATED SUMMARY:\n",
            "Dad is talking about his uncle Bill, his wife and two of their daughters in New Zealand. They want to travel to Europe next year and will visit them at the same Ae.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "==================== SamSum Prompt 4 ====================\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATED SUMMARY:\n",
            "Person1 wants Ms. Dawson to take dictation for him. The memo should be distributed to all employees by this afternoon.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# wants to adjust #Person1#'s life and #Person2# suggests #Person1# be positive and stay healthy.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATED SUMMARY:\n",
            "Person1 is worried about his future. He should get plenty of sleep, drink less wine and exercise.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person2# tells #Person1# about the relationships between their family and the uncle Bill's, who will visit them next year.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATED SUMMARY:\n",
            "Dad keeps talking about his family in New Zealand. His uncle Bill, his wife and two of their daughters are his cousins. Sarah and Jane are both his cousins although they are step-sisters. They want to travel to Europe next\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "==================== SamSum Prompt 5 ====================\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATED SUMMARY:\n",
            "Person1 wants to send a memo to all employees. It's about a new policy on communications. Employees who use Instant Messaging will be warned and placed on probation.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# wants to adjust #Person1#'s life and #Person2# suggests #Person1# be positive and stay healthy.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATED SUMMARY:\n",
            "Person1 is worried about his future. He doesn't know how to adjust his life.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person2# tells #Person1# about the relationships between their family and the uncle Bill's, who will visit them next year.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATED SUMMARY:\n",
            "Dad keeps talking about his family in New Zealand. They are his uncle Bill, his wife and two of their daughters. Sarah and Jane are his cousins. They want to travel to Europe next year and will visit us at the same Ae.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "==================== SamSum Prompt 6 ====================\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATED SUMMARY:\n",
            "The new policy on office communications is to be implemented. Employees are prohibited from using Instant Messaging.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# wants to adjust #Person1#'s life and #Person2# suggests #Person1# be positive and stay healthy.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATED SUMMARY:\n",
            "Person1 is worried about his future. He should get plenty of sleep, drink less wine and exercise.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person2# tells #Person1# about the relationships between their family and the uncle Bill's, who will visit them next year.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATED SUMMARY:\n",
            "Dad keeps talking about his family in New Zealand. His uncle Bill, his wife and two of their daughters are his cousins. Sarah and Jane are both his cousins, although they are step-sisters. They want to travel to Europe\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "==================== SamSum Prompt 7 ====================\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATED SUMMARY:\n",
            "The new policy on office communications is to be enforced. Employees who use Instant Messaging during working hours will be warned and placed on probation. Employees who continue to use Instant Messaging will face termination.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person1# wants to adjust #Person1#'s life and #Person2# suggests #Person1# be positive and stay healthy.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATED SUMMARY:\n",
            "Person1 is worried about his future. He doesn't know how to adjust his life.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN SUMMARY:\n",
            "#Person2# tells #Person1# about the relationships between their family and the uncle Bill's, who will visit them next year.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATED SUMMARY:\n",
            "Dad keeps talking about his family in New Zealand. His uncle Bill, his wife and two of their daughters are his cousins. Sarah and Jane are both his cousins, although they are step-sisters. They want to travel to Europe\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the prompts from Flan-T5 did help, but the model still struggles to pick up on the nuance of the conversation in some cases. This is what you will try to solve with few-shot inferencing."
      ],
      "metadata": {
        "id": "jKGDR4sxqs7X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Summarize Dialogues with a Few-Shot Inference\n",
        "\n",
        "**Few-shot inference** is the practice of providing an LLM with several examples of prompt-response pairs that match your task - before your actual prompt that you want completed. This is called \"in-context learning\" and puts your model into a state that understands your specific task."
      ],
      "metadata": {
        "id": "YsqbsMYSqvsV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise:** Build a function that takes a list of `in_context_example_indexes`, generates a prompt with the examples, then at the end appends the prompt that you want the model to complete (`test_example_index`). Use the same Flan-T5 prompt template from Section 3. Make sure to separate between the examples with `\"\\n\\n\\n\"`."
      ],
      "metadata": {
        "id": "EEexi_aSqwyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prompt(in_context_example_indices, test_example_index):\n",
        "    prompt = \"\"\n",
        "\n",
        "    for idx in in_context_example_indices:\n",
        "        example = dataset['test'][idx]\n",
        "        prompt += example[\"dialogue\"] + \"\\n\" + example[\"summary\"] + \"\\n\\n\\n\"\n",
        "\n",
        "    test_example = dataset['test'][test_example_index]\n",
        "    prompt += test_example[\"dialogue\"]\n",
        "\n",
        "    return prompt\n",
        "\n",
        "# Example usage\n",
        "in_context_example_indices = [0, 10, 20]\n",
        "test_example_index = 800\n",
        "\n",
        "few_shot_prompt = make_prompt(in_context_example_indices, test_example_index)\n",
        "print(few_shot_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TG8R59JVXjLt",
        "outputId": "85480462-e741-4553-9abd-491973a28327"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
            "#Person2#: Yes, sir...\n",
            "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
            "#Person2#: Yes, sir. Go ahead.\n",
            "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
            "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
            "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
            "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
            "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
            "#Person2#: This applies to internal and external communications.\n",
            "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
            "#Person2#: Is that all?\n",
            "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
            "Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.\n",
            "\n",
            "\n",
            "#Person1#: Happy Birthday, this is for you, Brian.\n",
            "#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n",
            "#Person1#: Brian, may I have a pleasure to have a dance with you?\n",
            "#Person2#: Ok.\n",
            "#Person1#: This is really wonderful party.\n",
            "#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n",
            "#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n",
            "#Person2#: You look great, you are absolutely glowing.\n",
            "#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n",
            "#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n",
            "\n",
            "\n",
            "#Person1#: What's wrong with you? Why are you scratching so much?\n",
            "#Person2#: I feel itchy! I can't stand it anymore! I think I may be coming down with something. I feel lightheaded and weak.\n",
            "#Person1#: Let me have a look. Whoa! Get away from me!\n",
            "#Person2#: What's wrong?\n",
            "#Person1#: I think you have chicken pox! You are contagious! Get away! Don't breathe on me!\n",
            "#Person2#: Maybe it's just a rash or an allergy! We can't be sure until I see a doctor.\n",
            "#Person1#: Well in the meantime you are a biohazard! I didn't get it when I was a kid and I've heard that you can even die if you get it as an adult!\n",
            "#Person2#: Are you serious? You always blow things out of proportion. In any case, I think I'll go take an oatmeal bath.\n",
            "#Person1# thinks #Person2# has chicken pox and warns #Person2# about the possible hazards but #Person2# thinks it will be fine.\n",
            "\n",
            "\n",
            "#Person1#: Dad, you keep talking about family in New Zealand. Who are they?\n",
            "#Person2#: Well, that's your uncle Bill, his wife and two of their daughters.\n",
            "#Person1#: Is uncle Bill your brother?\n",
            "#Person2#: No, your uncle Jack is my brother, Bill is my brother-in-law, your mom's brother.\n",
            "#Person1#: So his two daughters are my cousins?\n",
            "#Person2#: That's right, Sarah and Jane are both your cousins although they are step-sisters.\n",
            "#Person1#: What are step-sisters?\n",
            "#Person2#: Sarah is your uncle Bill's older daughter. When she was young, Bill's first wife, Sarah's mom died. Three years later Bill married again.\n",
            "#Person1#: So uncle Bill's wife is Jane's mother but not Sarah's mother. Right?\n",
            "#Person2#: Yes. She is Sarah's step-mother.\n",
            "#Person1#: And when are they coming to visit us?\n",
            "#Person2#: They want to travel to Europe next year, and will visit us at the same Ae.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now pass this prompt to the model perform a few shot inference:"
      ],
      "metadata": {
        "id": "C8t_GyFLeTSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary_for_test_example(in_context_example_indices, test_example_index):\n",
        "    few_shot_prompt = make_prompt(in_context_example_indices, test_example_index)\n",
        "\n",
        "    inputs = tokenizer(few_shot_prompt, return_tensors='pt', truncation=True, padding='longest')\n",
        "\n",
        "    summary_ids = model.generate(inputs['input_ids'], max_new_tokens=50, num_beams=4, early_stopping=True)\n",
        "\n",
        "    generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return generated_summary"
      ],
      "metadata": {
        "id": "ip7XkurGehCJ"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_summary = generate_summary_for_test_example(in_context_example_indices, test_example_index)\n",
        "print(\"Generated Summary:\")\n",
        "print(generated_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbBEA2owf5rP",
        "outputId": "b26fde5d-454a-4dd1-c60d-d1ea41c6a056"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Summary:\n",
            "Brian is having a birthday party. He wants to dance with Person2 at the party.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise:** Experiment with the few-shot inferencing:\n",
        "- Choose different dialogues - change the indices in the `in_context_example_indices` list and `test_example_index` value.\n",
        "- Change the number of examples. Be sure to stay within the model's 512 context length, however.\n",
        "\n",
        "How well does few-shot inference work with other examples?"
      ],
      "metadata": {
        "id": "6Ym0sAvRhgNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "in_context_example_indices = [0, 1, 2]\n",
        "test_example_index = 8\n",
        "\n",
        "few_shot_prompt = make_prompt(in_context_example_indices, test_example_index)\n",
        "print(few_shot_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KayZpADThidS",
        "outputId": "be16c8e1-7293-416c-e2e8-3844896d8679"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
            "#Person2#: Yes, sir...\n",
            "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
            "#Person2#: Yes, sir. Go ahead.\n",
            "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
            "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
            "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
            "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
            "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
            "#Person2#: This applies to internal and external communications.\n",
            "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
            "#Person2#: Is that all?\n",
            "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
            "Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.\n",
            "\n",
            "\n",
            "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
            "#Person2#: Yes, sir...\n",
            "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
            "#Person2#: Yes, sir. Go ahead.\n",
            "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
            "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
            "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
            "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
            "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
            "#Person2#: This applies to internal and external communications.\n",
            "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
            "#Person2#: Is that all?\n",
            "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
            "In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.\n",
            "\n",
            "\n",
            "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
            "#Person2#: Yes, sir...\n",
            "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
            "#Person2#: Yes, sir. Go ahead.\n",
            "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
            "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
            "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
            "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
            "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
            "#Person2#: This applies to internal and external communications.\n",
            "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
            "#Person2#: Is that all?\n",
            "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
            "Ms. Dawson takes a dictation for #Person1# about prohibiting the use of Instant Message programs in the office. They argue about its reasonability but #Person1# still insists.\n",
            "\n",
            "\n",
            "#Person1#: Kate, you never believe what's happened.\n",
            "#Person2#: What do you mean?\n",
            "#Person1#: Masha and Hero are getting divorced.\n",
            "#Person2#: You are kidding. What happened?\n",
            "#Person1#: Well, I don't really know, but I heard that they are having a separation for 2 months, and filed for divorce.\n",
            "#Person2#: That's really surprising. I always thought they are well matched. What about the kids? Who get custody?\n",
            "#Person1#: Masha, it seems quiet and makable, no quarrelling about who get the house and stock and then contesting the divorce with other details worked out.\n",
            "#Person2#: That's the change from all the back stepping we usually hear about. Well, I still can't believe it, Masha and Hero, the perfect couple. When would they divorce be final?\n",
            "#Person1#: Early in the New Year I guess.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_summary = generate_summary_for_test_example(in_context_example_indices, test_example_index)\n",
        "print(\"Generated Summary:\")\n",
        "print(generated_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atOZW17whxoU",
        "outputId": "815f2557-acd2-402f-b1ca-9ff404266d15"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Summary:\n",
            "Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "in_context_example_indices = [2, 23, 64, 5, 13]\n",
        "test_example_index = 22\n",
        "\n",
        "few_shot_prompt = make_prompt(in_context_example_indices, test_example_index)\n",
        "print(few_shot_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5cspw_Fh_w1",
        "outputId": "5d1a9625-8ebd-409d-8a52-b1ba29995d42"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
            "#Person2#: Yes, sir...\n",
            "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
            "#Person2#: Yes, sir. Go ahead.\n",
            "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
            "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
            "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
            "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
            "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
            "#Person2#: This applies to internal and external communications.\n",
            "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
            "#Person2#: Is that all?\n",
            "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
            "Ms. Dawson takes a dictation for #Person1# about prohibiting the use of Instant Message programs in the office. They argue about its reasonability but #Person1# still insists.\n",
            "\n",
            "\n",
            "#Person1#: Good coming. What can I do for you?\n",
            "#Person2#: I'm in Room 309. I'm checking out today. Can I have my bill now?\n",
            "#Person1#: Certainly. Please wait a moment. Here you are.\n",
            "#Person2#: Thanks. Wait... What's this? The 30 dollar for?\n",
            "#Person1#: Excuse me... The charge for your laundry service on Nov. 20th.\n",
            "#Person2#: But I did't take any laundry service during my stay here. I think you have added someone else's.\n",
            "#Person1#: Ummmm...Sorry, would you mind waiting a moment? We check it with the department concerned.\n",
            "#Person2#: No. As long as we get this straightened out.\n",
            "#Person1#: I'm very sorry. There has been a mistake. We'll correct the bill. Please take a look.\n",
            "#Person2#: Okay, here you are.\n",
            "#Person1#: Goodbye.\n",
            "#Person2# finds #Person2# being mischarged. #Person1# corrects the bill and #Person2# pays for it.\n",
            "\n",
            "\n",
            "#Person1#: Do you have any special skills?\n",
            "#Person2#: I can write computer programs, I have a good command of secretarial skills.\n",
            "#Person1#: What qualifications have you got?\n",
            "#Person2#: I have a doctor license and a driving license.\n",
            "#Person1#: Do you get special training in office skills?\n",
            "#Person2#: I passed both the Cambridge Examinations, First Certificate and the Certificate of Proficiency in English. And studied for a year in London at the Lucas Secretarial College, ending with speeds of 120 words per minute in English shorthand and 50 words per minute in typing. I was also trained in office procedure.\n",
            "#Person1#: Please tell me about work you have done, which qualifies you for this job.\n",
            "#Person2#: I'Ve received some special training in typing, shorthand and operating a fax machine, etc. . I'm experienced in IBM-PC. I can operate familiarly a word-processor, a fax machine, a photo and other office equipment.\n",
            "#Person2# is being interviewed by #Person1#. #Person2# is equipped with a bunch of valuable office skills.\n",
            "\n",
            "\n",
            "#Person1#: You're finally here! What took so long?\n",
            "#Person2#: I got stuck in traffic again. There was a terrible traffic jam near the Carrefour intersection.\n",
            "#Person1#: It's always rather congested down there during rush hour. Maybe you should try to find a different route to get home.\n",
            "#Person2#: I don't think it can be avoided, to be honest.\n",
            "#Person1#: perhaps it would be better if you started taking public transport system to work.\n",
            "#Person2#: I think it's something that I'll have to consider. The public transport system is pretty good.\n",
            "#Person1#: It would be better for the environment, too.\n",
            "#Person2#: I know. I feel bad about how much my car is adding to the pollution problem in this city.\n",
            "#Person1#: Taking the subway would be a lot less stressful than driving as well.\n",
            "#Person2#: The only problem is that I'm going to really miss having the freedom that you have with a car.\n",
            "#Person1#: Well, when it's nicer outside, you can start biking to work. That will give you just as much freedom as your car usually provides.\n",
            "#Person2#: That's true. I could certainly use the exercise!\n",
            "#Person1#: So, are you going to quit driving to work then?\n",
            "#Person2#: Yes, it's not good for me or for the environment.\n",
            "#Person2# complains to #Person1# about the traffic jam, #Person1# suggests quitting driving and taking public transportation instead.\n",
            "\n",
            "\n",
            "#Person1#: This Olympic park is so big!\n",
            "#Person2#: Yes. Now we are in the Olympic stadium, the center of this park.\n",
            "#Person1#: Splendid! When is it gonna be finished?\n",
            "#Person2#: The whole stadium is to be finished this June.\n",
            "#Person1#: How many seats are there in the stand?\n",
            "#Person2#: Oh, there are 5000 seats in total.\n",
            "#Person1#: I didn ' t know it would be so big!\n",
            "#Person2#: It is! Look there, those are the tracks. And the jumping pit is over there.\n",
            "#Person1#: Ah... I see. Hey, look the sign here, No climbing.\n",
            "#Person2#: We put many signs with English translations for foreign visitors.\n",
            "#Person2# shows #Person1# around the constructing Olympic stadium and introduces the stadium.\n",
            "\n",
            "\n",
            "#Person1#: Good coming. What can I do for you?\n",
            "#Person2#: I'm in Room 309. I'm checking out today. Can I have my bill now?\n",
            "#Person1#: Certainly. Please wait a moment. Here you are.\n",
            "#Person2#: Thanks. Wait... What's this? The 30 dollar for?\n",
            "#Person1#: Excuse me... The charge for your laundry service on Nov. 20th.\n",
            "#Person2#: But I did't take any laundry service during my stay here. I think you have added someone else's.\n",
            "#Person1#: Ummmm...Sorry, would you mind waiting a moment? We check it with the department concerned.\n",
            "#Person2#: No. As long as we get this straightened out.\n",
            "#Person1#: I'm very sorry. There has been a mistake. We'll correct the bill. Please take a look.\n",
            "#Person2#: Okay, here you are.\n",
            "#Person1#: Goodbye.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_summary = generate_summary_for_test_example(in_context_example_indices, test_example_index)\n",
        "print(\"Generated Summary:\")\n",
        "print(generated_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUEZ1k9WirR-",
        "outputId": "44301fa3-f4a7-47a4-ba26-dba375f88589"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Summary:\n",
            "The charge for the laundry service on Nov. 20th is 30 dollars.\n",
            "#Person1# helps #Person2# correct a mischarged bill on laundry service and helps #Person2# check out.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset['test'][22]['summary'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2RG67bTkZZE",
        "outputId": "0fc2d33b-decd-438a-c443-9cb3d639c24a"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Person1# helps #Person2# correct a mischarged bill on laundry service and helps #Person2# check out.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "few-shot learns better and the generated summary close to the human generated summaries. It performs better than no shot learning."
      ],
      "metadata": {
        "id": "xphwnLe5kb0H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Generative Configuration Parameters for Inference\n",
        "\n",
        "You can change the configuration parameters of the `generate()` method to see a different output from the LLM. So far the only parameter that you have been setting was `max_new_tokens=50`, which defines the maximum number of tokens to generate. A convenient way of organizing the configuration parameters is to use `GenerationConfig` class. By setting the parameter `do_sample = True`, you can activate various decoding strategies which influence the next token from the probability distribution over the entire vocabulary. You can then adjust the outputs changing `temperature` and other parameters (such as `top_k` and `top_p`). A full list of available parameters can be found in the [Hugging Face Generation documentation](https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationConfig)."
      ],
      "metadata": {
        "id": "woR_5bbppLEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(in_context_example_indices, test_example_index, max_new_tokens, num_beams, temperature, top_k, top_p):\n",
        "    few_shot_prompt = make_prompt(in_context_example_indices, test_example_index)\n",
        "\n",
        "    inputs = tokenizer(few_shot_prompt, return_tensors='pt', truncation=True, padding='longest')\n",
        "\n",
        "    summary_ids = model.generate(\n",
        "        inputs['input_ids'],\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        num_beams=num_beams,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return generated_summary\n",
        "\n",
        "in_context_example_indices = [0, 10, 20]\n",
        "test_example_index = 800\n",
        "\n",
        "configurations = [\n",
        "    {\"max_new_tokens\": 50, \"num_beams\": 4, \"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.9},\n",
        "    {\"max_new_tokens\": 100, \"num_beams\": 1, \"temperature\": 1.0, \"top_k\": 10, \"top_p\": 0.8},\n",
        "    {\"max_new_tokens\": 30, \"num_beams\": 8, \"temperature\": 0.3, \"top_k\": 100, \"top_p\": 0.95},\n",
        "    {\"max_new_tokens\": 70, \"num_beams\": 4, \"temperature\": 1.0, \"top_k\": 50, \"top_p\": 0.7}\n",
        "]\n",
        "\n",
        "for config in configurations:\n",
        "    generated_summary = generate_summary(\n",
        "        in_context_example_indices,\n",
        "        test_example_index,\n",
        "        max_new_tokens=config[\"max_new_tokens\"],\n",
        "        num_beams=config[\"num_beams\"],\n",
        "        temperature=config[\"temperature\"],\n",
        "        top_k=config[\"top_k\"],\n",
        "        top_p=config[\"top_p\"]\n",
        "    )\n",
        "    print(f\"\\nConfiguration: {config}\")\n",
        "    print(\"Generated Summary:\")\n",
        "    print(generated_summary)\n",
        "    print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCzjA_tepKAN",
        "outputId": "1d0e0275-a1b4-498f-eb1d-7e6651fd6af3"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Configuration: {'max_new_tokens': 50, 'num_beams': 4, 'temperature': 0.7, 'top_k': 50, 'top_p': 0.9}\n",
            "Generated Summary:\n",
            "Brian is having a birthday party. He wants to dance with Person2 at the party.\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:562: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:588: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Configuration: {'max_new_tokens': 100, 'num_beams': 1, 'temperature': 1.0, 'top_k': 10, 'top_p': 0.8}\n",
            "Generated Summary:\n",
            "Brian is having a birthday party. He wants to dance with Person2 at the party.\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:562: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `100` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Configuration: {'max_new_tokens': 30, 'num_beams': 8, 'temperature': 0.3, 'top_k': 100, 'top_p': 0.95}\n",
            "Generated Summary:\n",
            "Brian is having a birthday party. He wants to dance with Person2 at the party.\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Configuration: {'max_new_tokens': 70, 'num_beams': 4, 'temperature': 1.0, 'top_k': 50, 'top_p': 0.7}\n",
            "Generated Summary:\n",
            "Brian is having a birthday party. He wants to dance with Person2 at the party.\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The answer seems same for the for different configurations."
      ],
      "metadata": {
        "id": "i5UWe5fBty-I"
      }
    }
  ]
}