{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Author: Roi Yehoshua\n",
        "# Date: January 2024\n",
        "# MIT License\n",
        "\n",
        "# Based on the PyTorch implementation from https://nlp.seas.harvard.edu/annotated-transformer/\n"
      ],
      "metadata": {
        "id": "9JAB3VwE6YJF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking python version for compatibility with PyTorch"
      ],
      "metadata": {
        "id": "JXFcunDu7o8b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-O4ckyjvqTNz",
        "outputId": "42cbc298-b91f-4a5f-fc33-d965e0ec9d34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking the CUDA version for compatibilty with PyTorch"
      ],
      "metadata": {
        "id": "UTr_hXqH70Lz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfOmI3n4rR68",
        "outputId": "26f72c41-5e04-4ace-b40e-cacf4c9b9cfe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing versions of torch 2.1, torchdata 7.0 and torchtext 0.16 for compatibility of dependencies"
      ],
      "metadata": {
        "id": "TqEA7Uq18N2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.1.0+cu121 torchdata==0.7.0 torchtext==0.16.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tkrIJPiVrbPJ",
        "outputId": "2a5c74a0-3af0-433d-dc2c-ed178030b941"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==2.1.0+cu121\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.1.0%2Bcu121-cp310-cp310-linux_x86_64.whl (2200.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 GB\u001b[0m \u001b[31m438.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchdata==0.7.0\n",
            "  Downloading torchdata-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchtext==0.16.0\n",
            "  Downloading https://download.pytorch.org/whl/torchtext-0.16.0%2Bcpu-cp310-cp310-linux_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0+cu121) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0+cu121) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0+cu121) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0+cu121) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0+cu121) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0+cu121) (2023.6.0)\n",
            "Collecting triton==2.1.0 (from torch==2.1.0+cu121)\n",
            "  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.0) (2.0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.16.0) (4.66.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.16.0) (1.25.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0+cu121) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.7.0) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.7.0) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0+cu121) (1.3.0)\n",
            "Installing collected packages: triton, torch, torchdata, torchtext\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.3.0\n",
            "    Uninstalling triton-2.3.0:\n",
            "      Successfully uninstalled triton-2.3.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.0+cu121\n",
            "    Uninstalling torch-2.3.0+cu121:\n",
            "      Successfully uninstalled torch-2.3.0+cu121\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.18.0\n",
            "    Uninstalling torchtext-0.18.0:\n",
            "      Successfully uninstalled torchtext-0.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.3.0+cu121 requires torch==2.3.0, but you have torch 2.1.0+cu121 which is incompatible.\n",
            "torchvision 0.18.0+cu121 requires torch==2.3.0, but you have torch 2.1.0+cu121 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.1.0+cu121 torchdata-0.7.0 torchtext-0.16.0+cpu triton-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure you have the following packages installed:"
      ],
      "metadata": {
        "id": "fkE1KuwG8y1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "collapsed": true,
        "id": "baFAmAvNti3R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6893bdae-595e-4869-d18d-cdb7e6d8bbc5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.6.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install portalocker"
      ],
      "metadata": {
        "id": "tUR0lmSguvgk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2398df67-7268-415b-85ee-1fc47e2dcc84"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting portalocker\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking the downloaded versions of the torch, torchtext and torchdata"
      ],
      "metadata": {
        "id": "3S07bVjz9CJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import portalocker\n",
        "import torchdata\n",
        "import torchtext\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"torchdata version:\", torchdata.__version__)\n",
        "print(\"TorchText version:\", torchtext.__version__)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7FhQNdsgtbnV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b5e0a51-b963-4d63-91b8-5c9daf7afea6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.1.0+cu121\n",
            "torchdata version: 0.7.0\n",
            "TorchText version: 0.16.0+cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import spacy\n",
        "import os\n",
        "import portalocker\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import Multi30k\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "dCc19jv7trEn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)  # For reproducibility\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "id": "oRXVqXQZtwCx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21cde9a9-4642-488c-cc4a-35177b86d73b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-Head Attention\n",
        "\n",
        "$$\n",
        "    \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O \\\\\n",
        "    \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) \\\\  \n",
        "    \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "$$"
      ],
      "metadata": {
        "id": "FP9G5xc19_ib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"The multi-head attention module\"\"\"\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "\n",
        "        # Ensure the dimension of the model is divisible by the number of heads.\n",
        "        # This is necessary to equally divide the embedding dimension across heads.\n",
        "        assert d_model % num_heads == 0, 'd_model must be divisible by num_heads'\n",
        "\n",
        "        self.d_model = d_model           # Total dimension of the model\n",
        "        self.num_heads = num_heads       # Number of attention heads\n",
        "        self.d_k = d_model // num_heads  # Dimnsion of each head. We assume d_v = d_k\n",
        "\n",
        "        # Linear transformations for queries, keys, and values\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Final linear layer to project the concatenated heads' outputs back to d_model dimensions\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        ### WRITE YOUR CODE HERE\n",
        "\n",
        "        # 1. Calculate attention scores with scaling\n",
        "        d_k = Q.size(-1)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "        # 2. Apply mask (if provided) by setting masked positions to a large negative value.\n",
        "        if mask is not None:\n",
        "             scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # 3. Apply softmax to attention scores to get probabilities.\n",
        "        p_attn = scores.softmax(dim = -1)\n",
        "\n",
        "        weighted_sum = torch.matmul(p_attn, V)\n",
        "\n",
        "        # 4. Return the weighted sum of values based on attention probabilities.\n",
        "        return weighted_sum, p_attn\n",
        "\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        # Reshape the input tensor to [batch_size, num_heads, seq_length, d_k]\n",
        "        # to prepare for multi-head attention processing\n",
        "        batch_size, seq_length, d_model = x.size()\n",
        "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        # Inverse operation of split_heads: combine the head outputs back into the original tensor shape\n",
        "        # [batch_size, seq_length, d_model]\n",
        "        batch_size, num_heads, seq_length, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, num_heads*d_k)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        ### WRITE YOUR CODE HERE\n",
        "        # 1. Linearly project the queries, keys, and values, and then split them into heads.\n",
        "        Q, K, V = [\n",
        "            self.split_heads(lin(x)) for lin, x in zip(\n",
        "                [self.W_q, self.W_k, self.W_v], [Q, K, V]\n",
        "            )\n",
        "        ]\n",
        "\n",
        "\n",
        "        # 2. Apply scaled dot-product attention for each head.\n",
        "        x, self.attn = self.scaled_dot_product_attention(\n",
        "            Q, K, V, mask = mask\n",
        "        )\n",
        "\n",
        "\n",
        "        # 3. Concatenate the heads' outputs and apply the final linear projection.\n",
        "        x = self.combine_heads(x)\n",
        "\n",
        "\n",
        "\n",
        "        return self.W_o(x)\n"
      ],
      "metadata": {
        "id": "Tz1ItorctyVV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feed-Forward NN\n",
        "\n",
        "$$\n",
        "    \\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
        "$$"
      ],
      "metadata": {
        "id": "aQuWjf1HCr2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"\"\"The Positionwise Feedforward Network (FFN) module\"\"\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### WRITE YOUR CODE HERE\n",
        "        return self.linear2(self.dropout(self.linear1(x).relu()))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gqyCKZyNt0N2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional Encoding\n",
        "\n",
        "$$\n",
        "    \\text{PE}(pos, 2i) = \\sin(pos/10000^{2i/d_{\\text{model}}}) \\\\\n",
        "    \\text{PE}(pos, 2i + 1) = \\cos(pos/10000^{2i/d_{\\text{model}}})\n",
        "$$"
      ],
      "metadata": {
        "id": "lCfWOsx6C9XJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements the positional encoding module using sinusoidal functions of different frequencies\n",
        "    for each dimension of the encoding.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, max_seq_length):\n",
        "        super().__init__()\n",
        "\n",
        "        # Create a positional encoding (PE) matrix with dimensions [max_seq_length, d_model].\n",
        "        # This matrix will contain the positional encodings for all possible positions up to max_seq_length.\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "\n",
        "        # Generate a tensor of positions (0 to max_seq_length - 1) and reshape it to [max_seq_length, 1].\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        # Compute the division term used in the formulas for sin and cos functions.\n",
        "        # This term is based on the dimension of the model and the position, ensuring that the wavelengths\n",
        "        # form a geometric progression from 2π to 10000 * 2π. It uses only even indices for the dimensions.\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # Apply the sin function to even indices in the PE matrix. These values are determined by\n",
        "        # multiplying the position by the division term, creating a pattern where each position has\n",
        "        # a unique sinusoidal encoding.\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "\n",
        "        # Apply the cos function to odd indices in the PE matrix, complementing the sin-encoded positions.\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Register 'pe' as a buffer within the module. Unlike parameters, buffers are not updated during training.\n",
        "        # This is crucial because positional encodings are fixed and not subject to training updates.\n",
        "        # The unsqueeze(0) adds a batch dimension for easier broadcasting with input tensors.\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add positional encoding to the input tensor x.\n",
        "        # x is expected to have dimensions [batch_size, seq_length, d_model].\n",
        "        # The positional encoding 'pe' is sliced to match the seq_length of 'x', and then added to 'x'.\n",
        "        # This operation leverages broadcasting to apply the same positional encoding across the batch.\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x"
      ],
      "metadata": {
        "id": "sMmS5O9At4Pb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder Layer"
      ],
      "metadata": {
        "id": "UsY_r8kEEKjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\"An encoder layer consists of a multi-head self-attention sublayer and a feed forward sublayer,\n",
        "       with a dropout, residual connection, and layer normalization after each sub-layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
        "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        ### WRITE YOUR CODE HERE\n",
        "        self_attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = x + self.dropout(self_attn_output)\n",
        "        x = self.layer_norm1(x)\n",
        "\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = x+ self.dropout(ff_output)\n",
        "        x = self.layer_norm2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "juP4L2hct6kJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder Layer"
      ],
      "metadata": {
        "id": "k1Na0pxeEq0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \"\"\"A decoder layer consists of a multi-head self-attention, cross-attention and a feed-forward sublayers,\n",
        "       with a dropout, residual connection, and layer normalization after each sub-layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
        "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
        "        self.layer_norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        ### WRITE YOUR CODE HERE\n",
        "        self_attn_output = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = self.layer_norm1(x + self.dropout(self_attn_output))\n",
        "\n",
        "        cross_attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
        "        x = self.layer_norm2(x+self.dropout(cross_attn_output))\n",
        "\n",
        "        ff_output= self.feed_forward(x)\n",
        "        x = self.layer_norm3(x+ self.dropout(ff_output))\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "jwW-1kvwt8kL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Full Model"
      ],
      "metadata": {
        "id": "LGGkz668Eyyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements the Transformer model for sequence-to-sequence tasks such as machine translation.\n",
        "    The Transformer model, as described in \"Attention is All You Need\" by Vaswani et al., consists of an encoder and\n",
        "    decoder architecture that uses self-attention mechanisms to process input sequences and generate output sequences.\n",
        "\n",
        "    Parameters:\n",
        "    - src_vocab_size (int): Size of the source vocabulary.\n",
        "    - tgt_vocab_size (int): Size of the target vocabulary.\n",
        "    - d_model (int): Dimension of the model embeddings and hidden states.\n",
        "    - N (int): Number of layers in both the encoder and decoder stacks.\n",
        "    - n_heads (int): Number of attention heads in each multi-head attention mechanism.\n",
        "    - d_ff (int): Dimension of the feed-forward network within each layer.\n",
        "    - max_seq_length (int): Maximum length of input sequences, used for positional encoding.\n",
        "    - dropout (float): Dropout rate applied to embeddings and sub-layers.\n",
        "    - pad_idx (int): Index of the padding token in the source and target vocabularies.\n",
        "\n",
        "    Attributes:\n",
        "    - src_embedding (torch.nn.Embedding): Embedding layer for source sequences.\n",
        "    - tgt_embedding (torch.nn.Embedding): Embedding layer for target sequences.\n",
        "    - positional_encoding (PositionalEncoding): Adds positional information to embeddings.\n",
        "    - encoder (torch.nn.ModuleList): Stack of N encoder layers.\n",
        "    - decoder (torch.nn.ModuleList): Stack of N decoder layers.\n",
        "    - out (torch.nn.Linear): Linear layer that projects decoder output to target vocabulary size.\n",
        "    - dropout (torch.nn.Dropout): Dropout layer applied after embedding and positional encoding.\n",
        "\n",
        "    Methods:\n",
        "    - init_weights: Initializes model parameters using Glorot uniform initialization.\n",
        "    - create_source_mask: Creates a mask for padding tokens in the source sequence to ignore them in attention computations.\n",
        "    - create_target_mask: Creates combined padding and future token masks for the target sequence to prevent attending to future tokens and padding tokens.\n",
        "    - encode: Processes the source sequence through the encoder stack and generates memory states.\n",
        "    - decode: Processes the target sequence through the decoder stack using memory states from the encoder and applicable masks.\n",
        "    - forward: Defines the forward pass of the model using the encode and decode methods.\n",
        "    \"\"\"\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, N, n_heads, d_ff, max_seq_length, dropout, pad_idx):\n",
        "        super().__init__()\n",
        "\n",
        "        # Embedding layers for source and target\n",
        "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        # Encoder and Decoder stacks\n",
        "        self.encoder = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(N)])\n",
        "        self.decoder = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(N)])\n",
        "\n",
        "        # Output linear layer\n",
        "        self.out = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Initialization\n",
        "        self.init_weights()\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"Initialize parameters with Glorot / fan_avg\"\"\"\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def create_source_mask(self, src):\n",
        "        \"\"\"Create a mask for padding tokens in the source\"\"\"\n",
        "        src_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, src_len]\n",
        "        # unsqueeze(1) adds a dimension for the heads of the multi-head attention\n",
        "        # unsqueeze(2) adds a dimension for the attention scores\n",
        "        # This mask can be broadcasted across the src_len dimension of the attention scores,\n",
        "        # effectively masking out specific tokens across all heads and all positions in the sequence.\n",
        "        return src_mask\n",
        "\n",
        "    def create_target_mask(self, tgt):\n",
        "        \"\"\"Create masks for both padding tokens and future tokens\"\"\"\n",
        "        # Target padding mask\n",
        "        tgt_pad_mask = (tgt != self.pad_idx).unsqueeze(1).unsqueeze(3)  # [batch_size, 1, tgt_len, 1]\n",
        "        # unsqueeze(1) adds a dimension for the heads of the multi-head attention\n",
        "        # unsqueeze(3) adds a dimension for the attention scores\n",
        "        # The final shape allows the mask to be broadcast across the attention scores, ensuring positions only\n",
        "        # attend to allowed positions as dictated by the no-peak mask (the preceding positions) and the padding mask.\n",
        "\n",
        "        # Target no-peak mask\n",
        "        tgt_len = tgt.size(1)\n",
        "        tgt_nopeak_mask = torch.tril(torch.ones(tgt_len, tgt_len, device=device)).bool()\n",
        "\n",
        "        # Combine masks\n",
        "        tgt_mask = tgt_pad_mask & tgt_nopeak_mask  # [batch_size, 1, tgt_len, tgt_len]\n",
        "        return tgt_mask\n",
        "\n",
        "    def encode(self, src):\n",
        "        \"\"\"Encodes the source sequence using the Transformer encoder stack.\n",
        "        \"\"\"\n",
        "        src_mask = self.create_source_mask(src)\n",
        "        src = self.dropout(self.positional_encoding(self.src_embedding(src)))\n",
        "\n",
        "        # Pass through each layer in the encoder\n",
        "        for layer in self.encoder:\n",
        "            src = layer(src, src_mask)\n",
        "        return src, src_mask\n",
        "\n",
        "\n",
        "\n",
        "    def decode(self, tgt, memory, src_mask):\n",
        "        \"\"\"Decodes the target sequence using the Transformer decoder stack, given the memory from the encoder.\n",
        "        \"\"\"\n",
        "        tgt_mask = self.create_target_mask(tgt)\n",
        "        tgt = self.dropout(self.positional_encoding(self.tgt_embedding(tgt)))\n",
        "\n",
        "        # Pass through each layer in the decoder\n",
        "        for layer in self.decoder:\n",
        "            tgt = layer(tgt, memory, src_mask, tgt_mask)\n",
        "\n",
        "        # Output layer\n",
        "        output = self.out(tgt)\n",
        "        return output\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        ### WRITE YOUR CODE HERE\n",
        "        memory, src_mask = self.encode(src)\n",
        "        output = self.decode(tgt, memory, src_mask)\n",
        "\n",
        "        return output\n",
        "\n"
      ],
      "metadata": {
        "id": "-AFBw-zPt-YW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameters of the model\n",
        "src_vocab_size = 5000  # Size of source vocabulary\n",
        "tgt_vocab_size = 5000  # Size of target vocabulary\n",
        "d_model = 512          # Embedding dimension\n",
        "N = 6                  # Number of encoder and decoder layers\n",
        "num_heads = 8          # Number of attention heads\n",
        "d_ff = 2048            # Dimension of feed forward networks\n",
        "max_seq_length = 100   # Maximum sequence length\n",
        "dropout = 0.1          # Dropout rate\n",
        "pad_idx = 0            # Index of the padding token\n",
        "\n",
        "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, N, num_heads, d_ff, max_seq_length, dropout, pad_idx)\n",
        "\n",
        "# Move the model to the appropriate device (GPU or CPU)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "X4zlC09nuAsO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing on Random Data"
      ],
      "metadata": {
        "id": "saqL3AniFG0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate random sample data\n",
        "torch.manual_seed(42)\n",
        "\n",
        "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length)).to(device)  # (batch_size, seq_length)\n",
        "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length)).to(device)  # (batch_size, seq_length)"
      ],
      "metadata": {
        "id": "T5kIZVKyuBbY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "5y-oaC6yFM1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the next token using the first token in the first target tensor\n",
        "model.eval()\n",
        "\n",
        "memory, src_mask = model.encode(src_data[:1, :])\n",
        "output = model.decode(tgt_data[:1, :1], memory, src_mask)\n",
        "y = output.view(-1, tgt_vocab_size).argmax(-1)\n",
        "y"
      ],
      "metadata": {
        "collapsed": true,
        "id": "VdKCCtahuFiT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bceb8032-4c35-4531-faa0-e699918bb475"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([990], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct, you should get tensor([990])."
      ],
      "metadata": {
        "id": "tQE-bkQ1FT2v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "X0t9fYgQFhyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model for 10 epochs\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005, betas=(0.9, 0.98), eps=1e-9)\n",
        "grad_clip = 1\n",
        "n_epochs = 10\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    # Forward pass\n",
        "    output = model(src_data, tgt_data[:, :-1])\n",
        "\n",
        "    # tgt_data is of shape [batch_size, tgt_len]\n",
        "    # output is of shape [batch_size, tgt_len, tgt_vocab_size]\n",
        "    output = output.contiguous().view(-1, tgt_vocab_size)\n",
        "    tgt = tgt_data[:, 1:].contiguous().view(-1)\n",
        "    loss = criterion(output, tgt)\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    optimizer.step()\n",
        "    print(f'Epoch: {epoch + 1}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "63ByzNFHuKQN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9133ab78-85e8-4405-f82e-e7c8ba723462"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 8.605189323425293\n",
            "Epoch: 2, Loss: 8.501507759094238\n",
            "Epoch: 3, Loss: 8.37141227722168\n",
            "Epoch: 4, Loss: 8.296963691711426\n",
            "Epoch: 5, Loss: 8.23848819732666\n",
            "Epoch: 6, Loss: 8.192156791687012\n",
            "Epoch: 7, Loss: 8.16485595703125\n",
            "Epoch: 8, Loss: 8.142219543457031\n",
            "Epoch: 9, Loss: 8.13027286529541\n",
            "Epoch: 10, Loss: 8.12206745147705\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see the loss decreasing from around 8.6 to 8.1."
      ],
      "metadata": {
        "id": "nbaTxSykF2Vm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Machine Translation Example\n",
        "\n",
        "We now consider a real-world example using the Multi30k German-English translation task. This task is much smaller then the WMT task considered in the paper (only 30K sentence pairs compared to 4.5M pairs in the WMT-14 English-German dataset), but it illustrates the whole system.\n",
        "It is recommended to run this example on Google Colab, or on a machine with a strong GPU."
      ],
      "metadata": {
        "id": "miyaxjTzGAaF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Tokenizers"
      ],
      "metadata": {
        "id": "_j7eIorjG_YL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spacy models for tokenization\n",
        "try:\n",
        "    spacy_de = spacy.load('de_core_news_sm')\n",
        "except IOError:\n",
        "    os.system(\"python -m spacy download de_core_news_sm\")\n",
        "    spacy_de = spacy.load('de_core_news_sm')\n",
        "\n",
        "try:\n",
        "    spacy_en = spacy.load('en_core_web_sm')\n",
        "except IOError:\n",
        "    os.system(\"python -m spacy download en_core_web_sm\")\n",
        "    spacy_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "def tokenize_de(text):\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "def yield_tokens(data_iter, tokenizer, language):\n",
        "    for data_sample in data_iter:\n",
        "        yield tokenizer(data_sample[language])\n",
        "\n",
        "tokenizer_de = get_tokenizer(tokenize_de)\n",
        "tokenizer_en = get_tokenizer(tokenize_en)"
      ],
      "metadata": {
        "id": "lkFAh3RpuNTb"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Vocabularies"
      ],
      "metadata": {
        "id": "Zf8WqDRYHE6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, _, _ = Multi30k(split=('train', 'valid', 'test'))\n",
        "vocab_src = build_vocab_from_iterator(yield_tokens(train_data, tokenizer_de, 0),\n",
        "                                      specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
        "vocab_tgt = build_vocab_from_iterator(yield_tokens(train_data, tokenizer_en, 1),\n",
        "                                      specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
        "\n",
        "vocab_src.set_default_index(vocab_src['<unk>'])\n",
        "vocab_tgt.set_default_index(vocab_tgt['<unk>'])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6A325BZouPuu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70d4d249-6a92-4880-ae41-ed18876d0b7a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/combining.py:333: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the Transformer"
      ],
      "metadata": {
        "id": "oFqDEIQSHJht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameters of the model\n",
        "src_vocab_size = len(vocab_src)  # Size of source vocabulary\n",
        "tgt_vocab_size = len(vocab_tgt)  # Size of target vocabulary\n",
        "d_model = 512  # Embedding dimension\n",
        "N = 6          # Number of encoder and decoder layers\n",
        "num_heads = 8  # Number of attention heads\n",
        "d_ff = 2048    # Dimension of feed forward networks\n",
        "max_seq_length = 5000 # Maximum sequence length\n",
        "dropout = 0.1  # Dropout rate\n",
        "\n",
        "# Assume pad_idx is the padding index in the target vocabulary\n",
        "pad_idx = vocab_tgt['<pad>']\n",
        "\n",
        "# Initialize the Transformer model\n",
        "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, N, num_heads, d_ff, max_seq_length, dropout, pad_idx)\n",
        "\n",
        "# Move the model to the appropriate device (GPU or CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Hyperparameters for the training process\n",
        "batch_size = 128\n",
        "grad_clip = 1\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# Initialize the loss function with CrossEntropyLoss, ignoring the padding index\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
      ],
      "metadata": {
        "id": "JAhLUSzFv5XM"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Processing"
      ],
      "metadata": {
        "id": "ulqEaw_rHRR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_process(raw_data_iter):\n",
        "    data = []\n",
        "    for raw_src, raw_tgt in raw_data_iter:\n",
        "        src_tensor = torch.tensor([vocab_src[token] for token in tokenizer_de(raw_src)], dtype=torch.long)\n",
        "        tgt_tensor = torch.tensor([vocab_tgt[token] for token in tokenizer_en(raw_tgt)], dtype=torch.long)\n",
        "        data.append((src_tensor, tgt_tensor))\n",
        "    return data\n",
        "\n",
        "train_data, valid_data, test_data = Multi30k(split=('train', 'valid', 'test'))\n",
        "train_data = data_process(train_data)\n",
        "valid_data = data_process(valid_data)\n",
        "#test_data = data_process(test_data)\n",
        "# The test set of Multi30k is corrupted\n",
        "# See https://discuss.pytorch.org/t/unicodedecodeerror-when-running-test-iterator/192818/3"
      ],
      "metadata": {
        "id": "BbaMOmG6v7JA"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_batch(data_batch):\n",
        "    \"\"\"Processes a batch of source-target pairs by adding start-of-sequence (BOS) and end-of-sequence (EOS) tokens\n",
        "    to each sequence and padding all sequences to the same length.\n",
        "\n",
        "    Parameters:\n",
        "    - data_batch (Iterable[Tuple[Tensor, Tensor]]): A batch of source-target pairs, where each element is a tuple\n",
        "      containing the source sequence tensor and the target sequence tensor.\n",
        "    \"\"\"\n",
        "    src_batch, tgt_batch = [], []\n",
        "    src_batch, tgt_batch = [], []\n",
        "\n",
        "    # Iterate over each source-target pair in the provided batch\n",
        "    for src_item, tgt_item in data_batch:\n",
        "        # Prepend the start-of-sequence (BOS) token and append the end-of-sequence (EOS) token to the sequences\n",
        "        src_batch.append(torch.cat([torch.tensor([vocab_src['<bos>']]), src_item,\n",
        "                                    torch.tensor([vocab_src['<eos>']])], dim=0))\n",
        "        tgt_batch.append(torch.cat([torch.tensor([vocab_tgt['<bos>']]), tgt_item,\n",
        "                                    torch.tensor([vocab_tgt['<eos>']])], dim=0))\n",
        "\n",
        "    # Pad the sequences in the source batch to ensure they all have the same length.\n",
        "    # 'batch_first=True' indicates that the batch dimension should come first in the resulting tensor.\n",
        "    src_batch = pad_sequence(src_batch, padding_value=vocab_src['<pad>'], batch_first=True)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=vocab_tgt['<pad>'], batch_first=True)\n",
        "    return src_batch, tgt_batch\n",
        "\n",
        "# DataLoader for the training data, using the generate_batch function as the collate_fn.\n",
        "# This allows custom processing of each batch (adding BOS/EOS tokens and padding) before being fed into the model.\n",
        "train_iterator = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)\n",
        "\n",
        "# Similarly, DataLoader for the validation data\n",
        "valid_iterator = DataLoader(valid_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)"
      ],
      "metadata": {
        "id": "gjo1ajklv_Eh"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion, grad_clip):\n",
        "    \"\"\"\n",
        "    Trains the model for one epoch over the given dataset.\n",
        "    This function iterates over the provided data iterator, performing the forward and backward passes for each batch.\n",
        "    It employs teacher forcing by feeding the shifted target sequence (excluding the last token) as input to the decoder.\n",
        "\n",
        "    Parameters:\n",
        "    - model (torch.nn.Module): The model to be trained.\n",
        "    - iterator (Iterable): An iterable object that returns batches of data.\n",
        "    - optimizer (torch.optim.Optimizer): The optimizer to use for updating the model parameters.\n",
        "    - criterion (Callable): The loss function used to compute the difference between the model's predictions and the actual targets.\n",
        "    - grad_clip (float): The maximum norm of the gradients for gradient clipping.\n",
        "\n",
        "    Returns:\n",
        "    - float: The average loss for the epoch, computed as the total loss over all batches divided by the number of batches in the iterator.\n",
        "    \"\"\"\n",
        "    # Set the model to training mode.\n",
        "    # This enables dropout, layer normalization etc., which behave differently during training.\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    # Enumerate over the data iterator to get batches\n",
        "    for i, batch in enumerate(iterator):\n",
        "        # Unpack the batch to get source (src) and target (tgt) sequences\n",
        "        src, tgt = batch\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass through the model.\n",
        "        # For seq2seq models, the decoder input (tgt[:, :-1]) excludes the last token, implementing teacher forcing.\n",
        "        output = model(src, tgt[:, :-1])\n",
        "\n",
        "        # Reshape the output and target tensors to compute loss.\n",
        "        # The output tensor is reshaped to a 2D tensor where rows correspond to each token in the batch and columns to vocabulary size.\n",
        "\n",
        "        # tgt is of shape [batch_size, tgt_len]\n",
        "        # output is of shape [batch_size, tgt_len, tgt_vocab_size]\n",
        "        output = output.contiguous().view(-1, tgt_vocab_size)\n",
        "\n",
        "        # The target tensor is reshaped to a 1D tensor, excluding the first token (BOS) from each sequence.\n",
        "        tgt = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "        # Compute loss, perform backpropagation, and update model parameters\n",
        "        loss = criterion(output, tgt)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Compute average loss per batch for the current epoch\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "KA617MxjwMpg"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \"\"\"\n",
        "    Evaluates the model's performance on a given dataset.\n",
        "    This function is similar to the training loop, but without the backward pass and parameter updates.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src, tgt = batch\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "            output = model(src, tgt[:, :-1])\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            tgt = tgt[:, 1:].contiguous().view(-1)\n",
        "            loss = criterion(output, tgt)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "J3KeDuYiwQD2"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Model"
      ],
      "metadata": {
        "id": "axEWOccYHaLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 20\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, grad_clip)\n",
        "    val_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    print(f'\\nEpoch: {epoch + 1}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\tVal Loss: {val_loss:.3f}')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "YXUQgR5xwSP0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b55c2823-f90a-4217-ab4a-02fadf118f81"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "\tTrain Loss: 5.689\n",
            "\tVal Loss: 5.015\n",
            "\n",
            "Epoch: 2\n",
            "\tTrain Loss: 4.876\n",
            "\tVal Loss: 4.796\n",
            "\n",
            "Epoch: 3\n",
            "\tTrain Loss: 4.694\n",
            "\tVal Loss: 4.610\n",
            "\n",
            "Epoch: 4\n",
            "\tTrain Loss: 4.426\n",
            "\tVal Loss: 4.252\n",
            "\n",
            "Epoch: 5\n",
            "\tTrain Loss: 4.106\n",
            "\tVal Loss: 4.016\n",
            "\n",
            "Epoch: 6\n",
            "\tTrain Loss: 3.894\n",
            "\tVal Loss: 3.874\n",
            "\n",
            "Epoch: 7\n",
            "\tTrain Loss: 3.740\n",
            "\tVal Loss: 3.733\n",
            "\n",
            "Epoch: 8\n",
            "\tTrain Loss: 3.615\n",
            "\tVal Loss: 3.666\n",
            "\n",
            "Epoch: 9\n",
            "\tTrain Loss: 3.521\n",
            "\tVal Loss: 3.596\n",
            "\n",
            "Epoch: 10\n",
            "\tTrain Loss: 3.442\n",
            "\tVal Loss: 3.555\n",
            "\n",
            "Epoch: 11\n",
            "\tTrain Loss: 3.376\n",
            "\tVal Loss: 3.495\n",
            "\n",
            "Epoch: 12\n",
            "\tTrain Loss: 3.313\n",
            "\tVal Loss: 3.478\n",
            "\n",
            "Epoch: 13\n",
            "\tTrain Loss: 3.243\n",
            "\tVal Loss: 3.396\n",
            "\n",
            "Epoch: 14\n",
            "\tTrain Loss: 3.181\n",
            "\tVal Loss: 3.361\n",
            "\n",
            "Epoch: 15\n",
            "\tTrain Loss: 3.121\n",
            "\tVal Loss: 3.314\n",
            "\n",
            "Epoch: 16\n",
            "\tTrain Loss: 3.066\n",
            "\tVal Loss: 3.266\n",
            "\n",
            "Epoch: 17\n",
            "\tTrain Loss: 3.010\n",
            "\tVal Loss: 3.238\n",
            "\n",
            "Epoch: 18\n",
            "\tTrain Loss: 2.956\n",
            "\tVal Loss: 3.218\n",
            "\n",
            "Epoch: 19\n",
            "\tTrain Loss: 2.909\n",
            "\tVal Loss: 3.176\n",
            "\n",
            "Epoch: 20\n",
            "\tTrain Loss: 2.864\n",
            "\tVal Loss: 3.152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The train loss should decrease from around 5.7 to 2.8 after 20 epochs"
      ],
      "metadata": {
        "id": "ZFXn1MdNHeQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Translating a Sample Sentence"
      ],
      "metadata": {
        "id": "9O3cWKvyHmvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(model, sentence, vocab_src, vocab_tgt, max_length=50):\n",
        "    \"\"\"\n",
        "    Translates a given source sentence into the target language using a trained Transformer model.\n",
        "    The function preprocesses the input sentence by tokenizing and converting it to tensor format, then uses the model's\n",
        "    encode and decode methods to generate the translated sentence. The translation process is performed token by token\n",
        "    using greedy decoding, selecting the most likely next token at each step until an <eos> token is produced or the\n",
        "    maximum length is reached.\n",
        "\n",
        "    Parameters:\n",
        "    - model (torch.nn.Module): The trained Transformer model.\n",
        "    - sentence (str): The source sente         and '<eos>' (end of sentence).\n",
        "    - vocab_tgt (dict): The target vocabulary mapping of indices to tokens. It should provide a method `lookup_token`\n",
        "      to convert token indices back to                                                                                                                 the string representation.\n",
        "    - max_length (int, optional): The maximum allowed length for the generated translation. The decoding process will\n",
        "      stop when this length is reached if an <eos> token has not yet been generated.\n",
        "\n",
        "    Returns:\n",
        "    - str: The translated sentence as a string of text in the target language.\n",
        "    \"\"\"\n",
        "    ### WRITE YOUR CODE HERE\n",
        "    # Tokenize and convert the source sentence to tensor\n",
        "    src_tokens = [vocab_src[token] for token in sentence.split()]\n",
        "    src_tensor = torch.tensor(src_tokens).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    # Ensure both model and input are on the same device\n",
        "    device = next(model.parameters()).device  # Get the model's device\n",
        "    src_tensor = src_tensor.to(device)\n",
        "\n",
        "    # Encode the source sentence\n",
        "    encoder_outputs, src_mask = model.encode(src_tensor)\n",
        "\n",
        "    # Initialize the target sentence with the <sos> token\n",
        "    tgt_tokens = [vocab_tgt['<sos>']]\n",
        "    tgt_tensor = torch.tensor(tgt_tokens).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        # Decode the current target tensor\n",
        "        decoder_output = model.decode(tgt_tensor, encoder_outputs, src_mask)\n",
        "\n",
        "        # Get the last token's logit and find the most likely token\n",
        "        next_token = decoder_output[:, -1, :].argmax(dim=-1).item()\n",
        "\n",
        "        # Append the predicted token to the target tensor\n",
        "        tgt_tokens.append(next_token)\n",
        "        tgt_tensor = torch.tensor(tgt_tokens).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
        "\n",
        "        # If the end-of-sequence token is generated, stop decoding\n",
        "        if next_token == vocab_tgt['<eos>']:\n",
        "            break\n",
        "\n",
        "    # Convert the target tokens back to string\n",
        "    translated_sentence = ' '.join([vocab_tgt.lookup_token(idx) for idx in tgt_tokens if idx not in [vocab_tgt['<sos>'], vocab_tgt['<eos>']]])\n",
        "\n",
        "    return translated_sentence"
      ],
      "metadata": {
        "id": "Qi5cLhxCwc9M"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_sentence = \"Ein kleiner Junge spielt draußen mit einem Ball.\"  # German for \"A little boy playing outside with a ball.\"\n",
        "translated_sentence = translate_sentence(model, src_sentence, vocab_src, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')"
      ],
      "metadata": {
        "id": "NFNdqiVSQW-P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92ad3ea7-a79c-4392-b364-fccdfc44bd33"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: A young boy is playing in a pool .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should get a translation similar to the reference after 20 epochs of training."
      ],
      "metadata": {
        "id": "Xl6VyzkzHxRv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementation of Beam Search"
      ],
      "metadata": {
        "id": "SaAW3HoSH7Tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import heapq\n",
        "\n",
        "def beam_search(model, src_tensor, vocab_tgt, beam_width=3, max_length=50):\n",
        "    device = next(model.parameters()).device\n",
        "    src_tensor = src_tensor.to(device)\n",
        "\n",
        "    encoder_outputs, src_mask = model.encode(src_tensor)\n",
        "\n",
        "    # Initialize the beam with the start token\n",
        "    start_token = vocab_tgt['<sos>']\n",
        "    end_token = vocab_tgt['<eos>']\n",
        "\n",
        "    beams = [(0, [start_token])]\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        new_beams = []\n",
        "\n",
        "        for score, seq in beams:\n",
        "            tgt_tensor = torch.tensor(seq).unsqueeze(0).to(device)\n",
        "            decoder_output = model.decode(tgt_tensor, encoder_outputs, src_mask)\n",
        "\n",
        "            logits = decoder_output[:, -1, :]\n",
        "            topk = torch.topk(logits, beam_width, dim=-1)\n",
        "\n",
        "            for k in range(beam_width):\n",
        "                token_score = topk.values[0][k].item()\n",
        "                token = topk.indices[0][k].item()\n",
        "\n",
        "                new_score = score + token_score\n",
        "                new_seq = seq + [token]\n",
        "\n",
        "                new_beams.append((new_score, new_seq))\n",
        "\n",
        "        beams = sorted(new_beams, key=lambda x: x[0], reverse=True)[:beam_width]\n",
        "\n",
        "        if all(seq[-1] == end_token for score, seq in beams):\n",
        "            break\n",
        "\n",
        "    best_seq = beams[0][1]\n",
        "\n",
        "    translated_sentence = ' '.join([vocab_tgt.lookup_token(idx) for idx in best_seq if idx not in [start_token, end_token]])\n",
        "\n",
        "    return translated_sentence\n",
        "\n",
        "# Example usage:\n",
        "src_sentence = \"Ein kleiner Junge spielt draußen mit einem Ball.\"\n",
        "src_tokens = [vocab_src[token] for token in src_sentence.split()]\n",
        "src_tensor = torch.tensor(src_tokens).unsqueeze(0)  # Add batch dimension\n",
        "translated_sentence = beam_search(model, src_tensor, vocab_tgt)\n",
        "print(f'Translated sentence: {translated_sentence}')\n"
      ],
      "metadata": {
        "id": "9XvlnJynH7f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94517f6f-3f23-4bb7-c6d6-7a142615b5c0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated sentence: A young boy is playing in a pool .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experimenting with various Hyperparameters:"
      ],
      "metadata": {
        "id": "MUUR5v_phnsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def experiment(N, num_heads, lr, batch_size, num_epochs=20):\n",
        "\n",
        "  model = Transformer(src_vocab_size, tgt_vocab_size, d_model, N, num_heads, d_ff, max_seq_length, dropout, pad_idx)\n",
        "  model = model.to(device)\n",
        "\n",
        "  optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "  train_iterator = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)\n",
        "\n",
        "  valid_iterator = DataLoader(valid_data, batch_size=batch_size, shuffle=True, collate_fn= generate_batch)\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, grad_clip)\n",
        "    val_loss = evaluate(model, valid_iterator, criterion)\n",
        "    print(f'\\nEpoch: {epoch +1}')\n",
        "    print(f'\\tTrain Loss: {train_loss: .3f}')\n",
        "    print(f'\\tval Loss: {val_loss: .3f}')\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "5EkvJwHZhnQY"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 10\n",
        "num_heads_list = [4, 16]\n",
        "N_list = [4, 10]\n",
        "lr_list = [0.0001]\n",
        "batch_size_list = [64, 256]\n",
        "\n",
        "for num_heads in num_heads_list:\n",
        "  for N in N_list:\n",
        "    for lr in lr_list:\n",
        "      for batch_size in batch_size_list:\n",
        "        print(f\"Running experiment with num_heads={num_heads}, N={N}, lr={lr}, batch_size={batch_size}\")\n",
        "        model = experiment(N, num_heads, lr, batch_size)"
      ],
      "metadata": {
        "id": "BmMkglYxnp2r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "942c3081-b27b-4360-9931-ab41acfbd166"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running experiment with num_heads=4, N=4, lr=0.0001, batch_size=64\n",
            "\n",
            "Epoch: 1\n",
            "\tTrain Loss:  5.280\n",
            "\tval Loss:  4.811\n",
            "\n",
            "Epoch: 2\n",
            "\tTrain Loss:  4.582\n",
            "\tval Loss:  4.298\n",
            "\n",
            "Epoch: 3\n",
            "\tTrain Loss:  4.100\n",
            "\tval Loss:  3.954\n",
            "\n",
            "Epoch: 4\n",
            "\tTrain Loss:  3.817\n",
            "\tval Loss:  3.742\n",
            "\n",
            "Epoch: 5\n",
            "\tTrain Loss:  3.643\n",
            "\tval Loss:  3.638\n",
            "\n",
            "Epoch: 6\n",
            "\tTrain Loss:  3.503\n",
            "\tval Loss:  3.522\n",
            "\n",
            "Epoch: 7\n",
            "\tTrain Loss:  3.378\n",
            "\tval Loss:  3.407\n",
            "\n",
            "Epoch: 8\n",
            "\tTrain Loss:  3.272\n",
            "\tval Loss:  3.334\n",
            "\n",
            "Epoch: 9\n",
            "\tTrain Loss:  3.175\n",
            "\tval Loss:  3.251\n",
            "\n",
            "Epoch: 10\n",
            "\tTrain Loss:  3.088\n",
            "\tval Loss:  3.207\n",
            "\n",
            "Epoch: 11\n",
            "\tTrain Loss:  3.006\n",
            "\tval Loss:  3.126\n",
            "\n",
            "Epoch: 12\n",
            "\tTrain Loss:  2.929\n",
            "\tval Loss:  3.060\n",
            "\n",
            "Epoch: 13\n",
            "\tTrain Loss:  2.856\n",
            "\tval Loss:  3.008\n",
            "\n",
            "Epoch: 14\n",
            "\tTrain Loss:  2.781\n",
            "\tval Loss:  2.926\n",
            "\n",
            "Epoch: 15\n",
            "\tTrain Loss:  2.705\n",
            "\tval Loss:  2.884\n",
            "\n",
            "Epoch: 16\n",
            "\tTrain Loss:  2.627\n",
            "\tval Loss:  2.838\n",
            "\n",
            "Epoch: 17\n",
            "\tTrain Loss:  2.542\n",
            "\tval Loss:  2.755\n",
            "\n",
            "Epoch: 18\n",
            "\tTrain Loss:  2.456\n",
            "\tval Loss:  2.676\n",
            "\n",
            "Epoch: 19\n",
            "\tTrain Loss:  2.368\n",
            "\tval Loss:  2.620\n",
            "\n",
            "Epoch: 20\n",
            "\tTrain Loss:  2.277\n",
            "\tval Loss:  2.536\n",
            "Running experiment with num_heads=4, N=4, lr=0.0001, batch_size=256\n",
            "\n",
            "Epoch: 1\n",
            "\tTrain Loss:  6.162\n",
            "\tval Loss:  5.119\n",
            "\n",
            "Epoch: 2\n",
            "\tTrain Loss:  4.962\n",
            "\tval Loss:  4.843\n",
            "\n",
            "Epoch: 3\n",
            "\tTrain Loss:  4.787\n",
            "\tval Loss:  4.710\n",
            "\n",
            "Epoch: 4\n",
            "\tTrain Loss:  4.606\n",
            "\tval Loss:  4.486\n",
            "\n",
            "Epoch: 5\n",
            "\tTrain Loss:  4.376\n",
            "\tval Loss:  4.273\n",
            "\n",
            "Epoch: 6\n",
            "\tTrain Loss:  4.189\n",
            "\tval Loss:  4.127\n",
            "\n",
            "Epoch: 7\n",
            "\tTrain Loss:  4.030\n",
            "\tval Loss:  3.994\n",
            "\n",
            "Epoch: 8\n",
            "\tTrain Loss:  3.904\n",
            "\tval Loss:  3.897\n",
            "\n",
            "Epoch: 9\n",
            "\tTrain Loss:  3.801\n",
            "\tval Loss:  3.824\n",
            "\n",
            "Epoch: 10\n",
            "\tTrain Loss:  3.715\n",
            "\tval Loss:  3.753\n",
            "\n",
            "Epoch: 11\n",
            "\tTrain Loss:  3.628\n",
            "\tval Loss:  3.696\n",
            "\n",
            "Epoch: 12\n",
            "\tTrain Loss:  3.550\n",
            "\tval Loss:  3.632\n",
            "\n",
            "Epoch: 13\n",
            "\tTrain Loss:  3.472\n",
            "\tval Loss:  3.584\n",
            "\n",
            "Epoch: 14\n",
            "\tTrain Loss:  3.405\n",
            "\tval Loss:  3.519\n",
            "\n",
            "Epoch: 15\n",
            "\tTrain Loss:  3.342\n",
            "\tval Loss:  3.479\n",
            "\n",
            "Epoch: 16\n",
            "\tTrain Loss:  3.281\n",
            "\tval Loss:  3.429\n",
            "\n",
            "Epoch: 17\n",
            "\tTrain Loss:  3.223\n",
            "\tval Loss:  3.387\n",
            "\n",
            "Epoch: 18\n",
            "\tTrain Loss:  3.162\n",
            "\tval Loss:  3.341\n",
            "\n",
            "Epoch: 19\n",
            "\tTrain Loss:  3.109\n",
            "\tval Loss:  3.308\n",
            "\n",
            "Epoch: 20\n",
            "\tTrain Loss:  3.056\n",
            "\tval Loss:  3.256\n",
            "Running experiment with num_heads=4, N=10, lr=0.0001, batch_size=64\n",
            "\n",
            "Epoch: 1\n",
            "\tTrain Loss:  5.775\n",
            "\tval Loss:  5.732\n",
            "\n",
            "Epoch: 2\n",
            "\tTrain Loss:  5.191\n",
            "\tval Loss:  6.580\n",
            "\n",
            "Epoch: 3\n",
            "\tTrain Loss:  5.133\n",
            "\tval Loss:  6.641\n",
            "\n",
            "Epoch: 4\n",
            "\tTrain Loss:  5.125\n",
            "\tval Loss:  7.109\n",
            "\n",
            "Epoch: 5\n",
            "\tTrain Loss:  5.109\n",
            "\tval Loss:  6.920\n",
            "\n",
            "Epoch: 6\n",
            "\tTrain Loss:  5.100\n",
            "\tval Loss:  6.782\n",
            "\n",
            "Epoch: 7\n",
            "\tTrain Loss:  5.092\n",
            "\tval Loss:  6.950\n",
            "\n",
            "Epoch: 8\n",
            "\tTrain Loss:  5.078\n",
            "\tval Loss:  7.071\n",
            "\n",
            "Epoch: 9\n",
            "\tTrain Loss:  5.045\n",
            "\tval Loss:  7.044\n",
            "\n",
            "Epoch: 10\n",
            "\tTrain Loss:  5.046\n",
            "\tval Loss:  6.985\n",
            "\n",
            "Epoch: 11\n",
            "\tTrain Loss:  5.064\n",
            "\tval Loss:  6.901\n",
            "\n",
            "Epoch: 12\n",
            "\tTrain Loss:  5.058\n",
            "\tval Loss:  7.037\n",
            "\n",
            "Epoch: 13\n",
            "\tTrain Loss:  5.103\n",
            "\tval Loss:  13.285\n",
            "\n",
            "Epoch: 14\n",
            "\tTrain Loss:  5.096\n",
            "\tval Loss:  12.804\n",
            "\n",
            "Epoch: 15\n",
            "\tTrain Loss:  5.087\n",
            "\tval Loss:  13.608\n",
            "\n",
            "Epoch: 16\n",
            "\tTrain Loss:  5.083\n",
            "\tval Loss:  12.591\n",
            "\n",
            "Epoch: 17\n",
            "\tTrain Loss:  5.077\n",
            "\tval Loss:  13.184\n",
            "\n",
            "Epoch: 18\n",
            "\tTrain Loss:  5.067\n",
            "\tval Loss:  13.520\n",
            "\n",
            "Epoch: 19\n",
            "\tTrain Loss:  5.053\n",
            "\tval Loss:  6.353\n",
            "\n",
            "Epoch: 20\n",
            "\tTrain Loss:  5.031\n",
            "\tval Loss:  6.680\n",
            "Running experiment with num_heads=4, N=10, lr=0.0001, batch_size=256\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 414.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 389.06 MiB is free. Process 2125 has 14.37 GiB memory in use. Of the allocated memory 13.40 GiB is allocated by PyTorch, and 856.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-8b63411de88c>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_size_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Running experiment with num_heads={num_heads}, N={N}, lr={lr}, batch_size={batch_size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-31-9ae60cdfee28>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(N, num_heads, lr, batch_size, num_epochs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_clip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\nEpoch: {epoch +1}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-b5f7294d8eb4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, grad_clip)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# Compute loss, perform backpropagation, and update model parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_clip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 414.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 389.06 MiB is free. Process 2125 has 14.37 GiB memory in use. Of the allocated memory 13.40 GiB is allocated by PyTorch, and 856.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    }
  ]
}